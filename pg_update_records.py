# pg_update_records.py
"""
Create schema + concurrency-safe tri-state update API for versioned time-series.

Key conventions:
 - At the Python API level, every updatable field is tri-state:
     _UNSET => leave unchanged (field not provided)
     None   => explicitly set SQL NULL (clear)
     value  => set to that value
 - Canonicalization:
     comment: empty or whitespace-only -> SQL NULL
     tags: empty iterable or tags that normalize away -> SQL NULL
     tags normalized -> strip/lower/dedupe/sort (treated as a set)
 - No-op updates (canonicalized new == canonicalized current) are skipped.
"""
from __future__ import annotations

import os
from dataclasses import dataclass
from typing import Any, Optional, Sequence, Iterable, List, Dict, Tuple
import uuid
import datetime as dt

import psycopg
from psycopg.rows import dict_row

# -----------------------------------------------------------------------------
# DDL (the schema you approved)
# -----------------------------------------------------------------------------
DDL = """
BEGIN;

-- ============================================================================
-- 1) runs_table
-- ----------------------------------------------------------------------------
-- One row per worflow execution.
-- Stores metadata about the workflow run itself.
-- ============================================================================

CREATE TABLE IF NOT EXISTS runs_table (
  -- Unique identifier for the run (generated by the API)
  run_id           uuid PRIMARY KEY,

  -- Identifier for the workflow that produced this run
  workflow_id      text NOT NULL,

  -- When the workflow started and finished
  run_start_time   timestamptz NOT NULL,
  run_finish_time  timestamptz,

  -- Automatically populated when the row is inserted
  inserted_at      timestamptz NOT NULL DEFAULT now()
);

-- Index to efficiently query runs for a given workflow, ordered by time
-- Typical query:
--   WHERE workflow_id = 'xyz'
--   ORDER BY run_start_time DESC
CREATE INDEX IF NOT EXISTS runs_workflow_start_idx
  ON runs_table (workflow_id, run_start_time DESC);


-- ============================================================================
-- 2) values_table
-- ----------------------------------------------------------------------------
-- Stores forecast values.
-- This table is VERSIONED:
--   - Updates create new rows
--   - Old rows are kept for audit/history
--   - Exactly one row is marked is_current = true per key
-- ============================================================================

CREATE TABLE IF NOT EXISTS values_table (
  -- Unique identifier for each version of a value
  version_id  bigserial PRIMARY KEY,

  -- Foreign key back to the run that produced these values
  run_id      uuid NOT NULL REFERENCES runs_table(run_id) ON DELETE CASCADE,

  -- Timestamp values are valid for
  valid_time  timestamptz NOT NULL,

  -- What kind of series this is:
  -- e.g. 'mean', 'quantile:0.5', 'scenario:1'
  series_key  text NOT NULL,

  -- The workflow output value itself (can be NULL)
  -- NOTE: nullable so that NULL can be a valid stored value.
  -- In the application update API, we use _UNSET to mean "leave unchanged"
  -- and None/NULL to mean "explicitly set to NULL".
  value       double precision,

  -- Optional human annotations
  -- NULL means "no comment". 
  -- Empty or whitespace-only strings are disallowed (see constraint below)
  comment     text,

  -- Optional tags
  -- NULL means "no tags"
  -- Empty arrays {} are disallowed (see constraint below)
  tags        text[],

  -- Audit metadata
  -- changed_by: user or service making the change
  -- change_time: automatically set when this version is created
  changed_by  text,
  change_time timestamptz NOT NULL DEFAULT now(),

  -- Exactly one row per (run_id, valid_time, series_key) should be "current"
  is_current  boolean NOT NULL DEFAULT true,

  -- --------------------------------------------------------------------------
  -- Canonicalization guardrails
  -- --------------------------------------------------------------------------
  -- We treat "" (or whitespace-only) comments as equivalent to NULL (no comment).
  -- Enforce that empty/whitespace-only strings cannot be stored.
  CONSTRAINT comment_not_empty
    CHECK (comment IS NULL OR length(btrim(comment)) > 0),

  -- We treat {} (empty array) tags as equivalent to NULL (no tags).
  -- Enforce that empty arrays cannot be stored; use NULL instead.
  CONSTRAINT tags_not_empty_array
    CHECK (tags IS NULL OR cardinality(tags) > 0)
);

-- ============================================================================
-- 3) Integrity & performance indexes
-- ============================================================================

-- Enforce that there is ONLY ONE current version per
-- (run_id, valid_time, series_key)
-- This is the key guardrail that makes versioning safe.
CREATE UNIQUE INDEX IF NOT EXISTS values_one_current_idx
  ON values_table (run_id, valid_time, series_key)
  WHERE is_current;

-- Speeds up time-window queries across many runs
CREATE INDEX IF NOT EXISTS values_valid_time_idx
  ON values_table (valid_time);

-- Speeds up fetching a full forecast run
CREATE INDEX IF NOT EXISTS values_run_time_idx
  ON values_table (run_id, valid_time);

-- GIN index for efficient tag-based filtering
-- Example:
--   WHERE tags @> ARRAY['icing']
CREATE INDEX IF NOT EXISTS values_tags_gin_idx
  ON values_table USING GIN (tags);


-- ============================================================================
-- 4) Convenience view: current_values_table
-- ----------------------------------------------------------------------------
-- This view exposes ONLY the current values.
-- Most application queries should use this view instead of values_table
-- to avoid accidentally mixing in historical rows.
-- ============================================================================

CREATE OR REPLACE VIEW current_values_table AS
SELECT *
FROM values_table
WHERE is_current = true;

COMMIT;
"""

# -----------------------------------------------------------------------------
# Sentinel + dataclasses
# -----------------------------------------------------------------------------
_UNSET = object()  # sentinel to mean "field not provided / leave unchanged"

@dataclass(frozen=True)
class RecordKey:
    run_id: uuid.UUID
    valid_time: dt.datetime
    series_key: str


@dataclass(frozen=True)
class RecordUpdate:
    """
    Tri-state update object.
      - field == _UNSET => leave unchanged
      - field == None   => explicitly set SQL NULL (clear)
      - field == value  => set to that concrete value
    """
    run_id: uuid.UUID
    valid_time: dt.datetime           # tz-aware
    series_key: str

    # Use Any so callers can pass floats, None, or the _UNSET sentinel
    value: Any = _UNSET                 # float | None | _UNSET
    comment: Any = _UNSET               # str   | None | _UNSET
    tags: Any = _UNSET                  # Sequence[str] | None | _UNSET

    changed_by: Optional[str] = None


@dataclass(frozen=True)
class RecordUpdateResult:
    key: RecordKey
    version_id: int


@dataclass(frozen=True)
class UpdateRecordsOutcome:
    updated: List[RecordUpdateResult]
    skipped_no_ops: List[RecordKey]


# -----------------------------------------------------------------------------
# Canonicalization helpers
# -----------------------------------------------------------------------------
def _normalize_tag(t: str) -> Optional[str]:
    """Normalize a single tag: strip whitespace, lowercase, drop empties."""
    if t is None:
        return None
    s = str(t).strip().lower()
    return s or None


def _canonicalize_tags_input(tags) -> Optional[List[str]]:
    """
    Canonicalize tags provided by caller.
      - _UNSET => should not be passed here (handled by caller)
      - None   => explicit clear -> return None
      - empty iterable -> explicit clear -> return None
      - otherwise normalize, dedupe, sort -> list[str]
    Returns:
      - None => canonical "no tags" (SQL NULL)
      - list  => canonical non-empty sorted list of tags
    """
    if tags is None:
        return None
    if tags is _UNSET:
        # defensive: caller should not call this with _UNSET
        return None

    # Accept dict (use keys), list/tuple/set, etc. Scalars also accepted.
    if isinstance(tags, dict):
        seq = list(tags.keys())
    else:
        try:
            seq = list(tags)
        except TypeError:
            # single scalar -> treat as single tag
            seq = [tags]

    if not seq:
        return None

    uniq = {_normalize_tag(x) for x in seq}
    uniq.discard(None)
    if not uniq:
        return None
    return sorted(uniq)


def _canonicalize_tags_stored(tags) -> Optional[List[str]]:
    """
    Canonicalize tags read from DB (defensive).
    DB should store either NULL or a non-empty text[] (constraint prevents empty arrays).
    """
    if tags is None:
        return None
    seq = list(tags)
    if not seq:
        return None
    uniq = {_normalize_tag(x) for x in seq}
    uniq.discard(None)
    if not uniq:
        return None
    return sorted(uniq)


def _canonicalize_comment_input(comment) -> Optional[str]:
    """
    Canonicalize comment provided by caller:
      - None => explicit clear -> None
      - empty/whitespace-only => canonical None
      - otherwise strip() and return non-empty string
    """
    if comment is None:
        return None
    s = str(comment).strip()
    return s or None


def _canonicalize_comment_stored(comment) -> Optional[str]:
    """Canonicalize comment read from DB (defensive)."""
    if comment is None:
        return None
    s = str(comment).strip()
    return s or None


# -----------------------------------------------------------------------------
# Update function (concurrency-safe)
# -----------------------------------------------------------------------------
def update_records(conn: psycopg.Connection, updates: Iterable[RecordUpdate]) -> UpdateRecordsOutcome:
    """
    Batch, concurrency-safe versioned updates for values_table using _UNSET sentinel.

    Rules:
      - For each key (run_id, valid_time, series_key):
          * Lock current row (if any) with SELECT ... FOR UPDATE
          * Compute merged canonical values with tri-state rules
          * If no current row: caller must provide `value` (explicitly; may be None)
          * If canonical new == canonical current => skip (no-op)
          * Otherwise: unset old current row (is_current=false) and insert new current row
      - Returns lists of updated rows and skipped no-op keys
    """
    updates_list = list(updates)
    if not updates_list:
        return UpdateRecordsOutcome(updated=[], skipped_no_ops=[])

    # Validate and collapse duplicates (last-write-wins)
    KeyT = Tuple[uuid.UUID, dt.datetime, str]
    collapsed: Dict[KeyT, RecordUpdate] = {}
    for u in updates_list:
        if u.valid_time.tzinfo is None:
            raise ValueError(f"valid_time must be timezone-aware (timestamptz). Bad update: {u}")
        # ensure at least one field is being modified
        if (u.value is _UNSET) and (u.comment is _UNSET) and (u.tags is _UNSET):
            raise ValueError(
                "No updates supplied: provide at least one of value/comment/tags (use _UNSET to leave fields unchanged)."
            )
        collapsed[(u.run_id, u.valid_time, u.series_key)] = u

    # Deterministic order reduces deadlock risk
    ordered_items = sorted(
        collapsed.items(),
        key=lambda kv: (str(kv[0][0]), kv[0][1].isoformat(), kv[0][2]),
    )

    sql_lock_current = """
        SELECT version_id, value, comment, tags
        FROM values_table
        WHERE run_id = %(run_id)s
          AND valid_time = %(valid_time)s
          AND series_key = %(series_key)s
          AND is_current = true
        FOR UPDATE
    """

    sql_unset_current = """
        UPDATE values_table
        SET is_current = false
        WHERE run_id = %(run_id)s
          AND valid_time = %(valid_time)s
          AND series_key = %(series_key)s
          AND is_current = true
    """

    sql_insert_new = """
        INSERT INTO values_table (
            run_id, valid_time, series_key,
            value, comment, tags,
            changed_by, is_current
        )
        VALUES (
            %(run_id)s, %(valid_time)s, %(series_key)s,
            %(value)s, %(comment)s, %(tags)s,
            %(changed_by)s, true
        )
        RETURNING version_id
    """

    updated: List[RecordUpdateResult] = []
    skipped: List[RecordKey] = []

    # Run batch inside a transaction to maintain invariants
    with conn.transaction():
        with conn.cursor(row_factory=dict_row) as cur:
            for (run_id, valid_time, series_key), u in ordered_items:
                key = RecordKey(run_id=run_id, valid_time=valid_time, series_key=series_key)

                # 1) Lock current row for this key (if it exists)
                cur.execute(sql_lock_current, {"run_id": run_id, "valid_time": valid_time, "series_key": series_key})
                current = cur.fetchone()

                # 2) Canonicalize current row defensively
                if current is None:
                    current_value = None
                    current_comment_canon = None
                    current_tags_canon = None
                else:
                    current_value = current["value"]     # may be None
                    current_comment_canon = _canonicalize_comment_stored(current["comment"])
                    current_tags_canon = _canonicalize_tags_stored(current["tags"])

                # 3) Merge using tri-state semantics
                # VALUE
                if u.value is _UNSET:
                    new_value = current_value
                else:
                    # explicit provided: may be None (clear) or a concrete numeric value
                    new_value = u.value

                # COMMENT
                if u.comment is _UNSET:
                    new_comment = current_comment_canon
                else:
                    # explicit provided: None => clear; string => canonicalize
                    new_comment = _canonicalize_comment_input(u.comment)

                # TAGS
                if u.tags is _UNSET:
                    new_tags = current_tags_canon
                else:
                    # explicit provided: None or empty -> clear (canonical None); else normalized list
                    new_tags = _canonicalize_tags_input(u.tags)

                # 4) If no current row exists, require `value` to be provided (explicitly, possibly None)
                if current is None and u.value is _UNSET:
                    raise ValueError(
                        f"No current row exists for {key}. You must provide `value` (explicitly, possibly None) to create the first version."
                    )

                # 5) No-op detection (compare canonicalized forms)
                if (new_value == current_value) and (new_comment == current_comment_canon) and (new_tags == current_tags_canon):
                    skipped.append(key)
                    continue  # nothing to do for this key

                # 6) Unset previous current row (0 or 1 rows)
                cur.execute(sql_unset_current, {"run_id": run_id, "valid_time": valid_time, "series_key": series_key})

                # 7) Insert new current version
                cur.execute(sql_insert_new, {
                    "run_id": run_id,
                    "valid_time": valid_time,
                    "series_key": series_key,
                    "value": new_value,
                    "comment": new_comment,
                    "tags": new_tags,        # None => SQL NULL; list => text[]
                    "changed_by": u.changed_by,
                })
                version_id = cur.fetchone()["version_id"]
                updated.append(RecordUpdateResult(key=key, version_id=version_id))

    return UpdateRecordsOutcome(updated=updated, skipped_no_ops=skipped)


# -----------------------------------------------------------------------------
# Schema creation helper
# -----------------------------------------------------------------------------
def create_schema(conninfo: str) -> None:
    """
    Creates (or updates) the database schema.

    - Uses an explicit transaction (BEGIN/COMMIT)
    - Safe to run multiple times on a fresh DB
    """
    with psycopg.connect(conninfo, autocommit=False) as conn:
        with conn.cursor() as cur:
            cur.execute(DDL)


# -----------------------------------------------------------------------------
# Example usage (run as script)
# -----------------------------------------------------------------------------
if __name__ == "__main__":
    load_dotenv()
    conninfo = os.environ.get("NEON_PG_URL")
    if not conninfo:
        raise RuntimeError("NEON_PG_URL must be set in environment for example run")

    # Create schema (safe for fresh DBs)
    create_schema(conninfo)
    print("Schema created/updated.")

    # Demo of update_records
    with psycopg.connect(conninfo) as conn:
        # Create a run to reference
        run_id = uuid.uuid4()
        with conn.cursor() as cur:
            cur.execute(
                "INSERT INTO runs_table (run_id, workflow_id, run_start_time) VALUES (%s, %s, %s)",
                (run_id, "demo-workflow", dt.datetime.now(tz=dt.timezone.utc)),
            )

        t = dt.datetime(2025, 12, 27, 12, 0, tzinfo=dt.timezone.utc)

        # 1) Create first version (value provided; may be None explicitly)
        upd = RecordUpdate(
            run_id=run_id,
            valid_time=t,
            series_key="quantile:0.5",
            value=123.4,
            comment="initial value",
            tags=["Icing", "Review"],
            changed_by="demo",
        )
        out = update_records(conn, [upd])
        print("Updated:", out.updated)
        print("Skipped:", out.skipped_no_ops)

        # 2) Update only comment (leave value/tags unchanged)
        upd2 = RecordUpdate(
            run_id=run_id,
            valid_time=t,
            series_key="quantile:0.5",
            comment="  checked  ",   # trimmed to "checked"
            # value/tags left as _UNSET => unchanged
            changed_by="demo",
        )
        out2 = update_records(conn, [upd2])
        print("Updated:", out2.updated)
        print("Skipped:", out2.skipped_no_ops)

        # 3) Clear tags explicitly
        upd3 = RecordUpdate(
            run_id=run_id,
            valid_time=t,
            series_key="quantile:0.5",
            tags=[],   # explicit clear -> stored as NULL (canonical)
            changed_by="demo",
        )
        out3 = update_records(conn, [upd3])
        print("Updated:", out3.updated)
        print("Skipped:", out3.skipped_no_ops)

